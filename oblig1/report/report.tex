\documentclass[a4paper,11pt]{article}
%\documentclass[preprint]{aa}

%\documentclass[preprint]{aastex}

%\documentclass[journal = ancham]{achemso}
%\setkeys{acs}{useutils = true}
%\usepackage{fullpage}
\usepackage{natbib,twoopt}
\pretolerance=2000
\tolerance=6000
\hbadness=6000
%\usepackage[landscape]{geometry}
%\usepackage{pxfonts}
%\usepackage{cmbright}
%\usepackage[varg]{txfonts}
%\usepackage{mathptmx}
%\usepackage{tgtermes}
\usepackage[utf8]{inputenc}
%\usepackage{fouriernc}
%\usepackage[adobe-utopia]{mathdesign}
\usepackage[T1]{fontenc}
%\usepackage[norsk]{babel}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage[version=3]{mhchem}
\usepackage{pstricks}
\usepackage[font=small,labelfont=bf,tableposition=below]{caption}
%\usepackage{subfig}
\usepackage{subcaption}
%\usepackage{varioref}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{sverb}
%\usepackage{microtype}
%\usepackage{enumerate}
\usepackage{enumitem}
%\usepackage{lineno}
%\usepackage{booktabs}
%\usepackage{changepage}
%\usepackage[flushleft]{threeparttable}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{mathtools}
%\usepackage{etoolbox}
%\usepackage{xstring}
\usepackage{aas_macros}

\floatstyle{plaintop}
\restylefloat{table}
%\floatsetup[table]{capposition=top}

\setcounter{secnumdepth}{3}

\newcommand{\tr}{\, \text{tr}\,}
\newcommand{\diff}{\ensuremath{\; \text{d}}}
\newcommand{\diffd}{\ensuremath{\text{d}}}
\newcommand{\sgn}{\ensuremath{\; \text{sgn}}}
\newcommand{\UA}{\ensuremath{_{\uparrow}}}
\newcommand{\RA}{\ensuremath{_{\rightarrow}}}
\newcommand{\QED}{\left\{ \hfill{\textbf{QED}} \right\}}

%% The below macros turn citations into ADS clickers in dvi, pdf, html output.
%% EDP Sciences improved them in December 2012 to work also with pdflatex.
\bibpunct{(}{)}{;}{a}{}{,}    %% natbib cite format used by A&A and ApJ
\makeatletter
 \newcommandtwoopt{\citeads}[3][][]{\href{http://adsabs.harvard.edu/abs/#3}%
   {\def\hyper@linkstart##1##2{}%
    \let\hyper@linkend\@empty\citealp[#1][#2]{#3}}}    %% Rutten, 2000
 \newcommandtwoopt{\citepads}[3][][]{\href{http://adsabs.harvard.edu/abs/#3}%
   {\def\hyper@linkstart##1##2{}%
    \let\hyper@linkend\@empty\citep[#1][#2]{#3}}}      %% (Rutten 2000)
 \newcommandtwoopt{\citetads}[3][][]{\href{http://adsabs.harvard.edu/abs/#3}%
   {\def\hyper@linkstart##1##2{}%
    \let\hyper@linkend\@empty\citet[#1][#2]{#3}}}      %% Rutten (2000)
 \newcommandtwoopt{\citeyearads}[3][][]%
   {\href{http://adsabs.harvard.edu/abs/#3}%
   {\def\hyper@linkstart##1##2{}%
    \let\hyper@linkend\@empty\citeyear[#1][#2]{#3}}}   %% 2000
\makeatother

%\newcommand{\diff}{%
%    \IfEqCase{frac{\diff}{%
%        {\ensuremath{frac{\text{d}} }}%
%        {\ensuremath{\; \text{d}} }% 
%    }[\PackageError{diff}{Problem with diff}{}]%
%}%


\date{\today}
\title{Compulsory assignment spring 2014\\ \small{Statistical mechanics -- FYS4130}}
\author{Marius Berge Eide \\ \texttt{m.b.eide@astro.uio.no}}


\begin{document}


\onecolumn
\maketitle{}


\section{Part 1}

\begin{enumerate}
    \item \textbf{Normalisation and variance}

        The distribution $P_0(x)$ is normalised;
        \begin{align*}
            \int_{-\infty}^{+\infty} P_0(x) \diff x &= \int_{-1}^{+1} \frac{1}{2} \diff x \\
            &= \frac{1}{2} \left[ 1 - \left( -1 \right) \right] = 1
        \end{align*}
        and the variance $\langle \Delta x^2 \rangle$ can be found as $\langle \Delta x^2 \rangle - \langle \Delta x \rangle^2$ with
        \begin{align*}
            \langle x \rangle &= \int_{-\infty}^{+\infty} x P_0(x) \diff x = \int_{-1}^{+1} x \frac{1}{2} \diff x \\
            &= \frac{1}{2} \left[ \frac{1}{2} \left( 1^2 - (-1)^2 \right) \right] = 0
        \end{align*}
        and
        \begin{align*}
            \langle x^2 \rangle &=  \int_{-\infty}^{+\infty} x^2 P_0(x) \diff x = \int_{-1}^{+1} x^2 \frac{1}{2} \diff x \\
            &= \frac{1}{2} \left[ \frac{1}{3} \left( 1^3 - (-1)^3 \right) \right] \\
            &= \frac{1}{6} 2 = \frac{1}{3}
        \end{align*}
        giving
        \begin{align}
            \langle \Delta x^2 \rangle &= \langle \Delta x^2 \rangle - \langle \Delta x \rangle^2 \notag \\
            &= \frac{1}{3} - 0 = \frac{1}{3}.
            \label{eq:variance1}
        \end{align}

    \item \textbf{Random sequence}
        A sequence of $N=2^{\left\{1,2,\dots,16  \right\}}$ $X$-values are produced\footnote{see the attached file \texttt{p1\_rng.py}}. The variance is plotted in fig.~(\ref{fig:12_variance}). 

        \begin{figure}[htb]
            \begin{center}
                \includegraphics[width=\columnwidth]{../12_variance.pdf}
            \end{center}
            \caption{Variance of random numbers $X$ produced by a random number generator in the range $[-1,1)$ following the flat distribution $P_0(x)$, plotted against number of random numbers $N$. Note how the variance stabilises with increasing $N$. }
            \label{fig:12_variance}
        \end{figure}

    \item \textbf{Histogram} A histogram showing the distribution of $N=2^{16}$ random numbers $X$ is plotted in fig.~(\ref{fig:histogram}). The central limit theorem states that, for high $N$, any distribution can be approximated by a normal distribution with variance 

        \begin{figure}[htb]
            \begin{center}
                \includegraphics[width=\columnwidth]{../13_histogram.pdf}
            \end{center}
            \caption{Histogram showing the distribution of $N=2^{^16}$ random numbers $X$ generated under the flat distribution $P_0(x)$. The mean is zero to two decimals, and the variance is $\langle \Delta X^2 \rangle = 0.334094$.  }
            \label{fig:13_histogram}
        \end{figure}<++>
\end{enumerate}<++>

\section{Part 2 -- The central limit theorem}
The first distribution to be examined is the power law approximation to the log-normal distribution;
\begin{equation}
    P(x) \propto \frac{1}{x}
    \label{eq:powerlawdist}
\end{equation}

\begin{enumerate}
    \item The distribution is not normalisable as
        \begin{equation}
            \int_{-\infty}^{+\infty} \frac{1}{x} \diff x = \infty
            \label{eq:diverging_dist}
        \end{equation}
        in other words, the integral diverges, and for that no normalisation constant exists. For $|x| \to 0$, $1/|x| \to \infty$, and for $|x| \to \infty$, $1/|x| \to 0$, so that cut off values must be introduced when implementing the distribution numerically (when is $|x|$ small enough to make $1/|x|$ approximately infinitely large, and inversely likewise for large $|x|$?).

    \item For a random variable $y$ distributed following the flat distribution $P_0(y)$ in between $-1$ and $+1$, a change of variable $y \to x$ must not change the probability over an infinitesimal small step;
        \begin{equation}
            P_0(y) \diffd y = P(x) \diffd x
            \label{eq:consv_prob}
        \end{equation}
        so that either distribution can be expressed in terms of the other,
        \begin{align}
            P(x) &= P_0(y) \frac{\diffd y}{\diffd x} \notag \\
            &= \frac{P_0(y)}{x'(y)}
            \label{eq:expdist_distilled}
        \end{align}
        where the prime ($'$) denotes derivative with respect to $y$, ie., $' \equiv \diffd / \diffd y$. 

    \item Using the change of variable $x = Be^{Ay}$ and the relation of eq.~(\ref{eq:expdist_distilled}), the $x$-dependent distribution can be written as
        \begin{align}
            P(x) &= \frac{P_0(y(x))}{x'(y)} = \frac{1}{2} \frac{1}{Ax},
            \label{eq:px}
        \end{align}
        where it was used that $x'(y) = \diffd (Be^{Ay})/\diffd y = ABe^{Ay} = Ax$. 

        The valid range for the distribution follows from the endpoints of the flat distribution. The minimum is given by $x(y=-1) = Be^{-A}$, and the maximum is given by $x(y=+1) = Be^{+A}$. For other $y$, the distribution is zero.

    \item The distribution of eq.~(\ref{eq:px}) is normalised,
        \begin{align*}
            \int_{-\infty}^{\infty} P(x) \diff x &= \int_{Be^{-A}}^{Be^{+A}} \frac{1}{2Ax} \diff x \notag \\
            &=  \frac{1}{2A}\int_{Be^{-A}}^{Be^{+A}} \frac{1}{x} \diff x = \frac{1}{2A} \left[ \ln \left( Be^{+A} \right) - \ln \left( Be^{-A} \right) \right] \notag \\
            &=  \frac{1}{2A} \left( A - (-A) \right) = 1
        \end{align*}
        and the variance can be found from $\langle x \rangle$, $\langle x^2 \rangle$;
        \begin{align*}
            \langle x \rangle &= \int_{-\infty}^{\infty} x P(x) \diff x = \int_{Be^{-A}}^{Be^{+A}} x \frac{1}{2Ax} \diff x \\
            &=\frac{1}{2A}\int_{Be^{-A}}^{Be^{+A}} \diff x = \frac{B}{2A} \left( e^{+A} - e^{-A}  \right) \\
            &= \frac{B}{A} \sinh A
        \end{align*}
        and
        \begin{align*}
            \langle x^2 \rangle &= \int_{-\infty}^{\infty} x^2 P(x) \diff x = \int_{Be^{-A}}^{Be^{+A}} x^2 \frac{1}{2Ax} \diff x \\
            &=\frac{1}{2A}\int_{Be^{-A}}^{Be^{+A}} x \diff x = \frac{1}{2A} \left[ \frac{1}{2} (Be^{+A})^2 - \frac{1}{2}(Be^{-A})^2  \right] \\
            &= \frac{B^2}{4A} \left[ e^{2A} - e^{-2A} \right] = \frac{B^2}{2A} \sinh \left( 2A \right) \\
            &= \frac{B^2}{A} \cosh A \sinh A
        \end{align*}
        giving the variance 
        \begin{align}
        \langle \Delta x^2 \rangle &= \langle x^2 \rangle - \langle x \rangle^2 \notag \\
        &= \frac{B^2}{A} \cosh A \sinh A - \frac{B^2}{A^2} \sinh^2 A \notag \\
        &= \frac{B^2}{A} \sinh A \left( \cosh A - \frac{1}{A} \sinh A \right)
            \label{eq:var_1}
        \end{align}

    \item The variance can be rewritten
        \begin{align}
            \langle \Delta x^2 \rangle &= \frac{B^2}{A^2} \sinh^2 A \left( A \frac{\cosh A}{\sinh A} -\frac{\sinh A}{\sinh A} \right) \notag \\
            &= \frac{B^2}{A^2} \sinh^2 A \left( A \coth A - 1 \right)
            \label{eq:var_2}
        \end{align}
        as $\tanh A = 1/\coth A$. 

    \item Seqence of X values
    \item Other parameters
    \item Other params, running average. Symmetric histogram?
    \item Considering a random walk where each step follows $P(x)$ from eq.~(\ref{eq:px}), with independent steps, the mean-sqaure displacement can be found by using known features of the flat distribution $P_0(y)$. In the interval $[-1,1]$, the probability of going left or right is independent of location (and thus previous steps) and given by $P_0(y) = 1/2$. Either you go left, or you go right.

        The change of variable allows us to retain the image of going left or right, but within a different interval and following a different distribution. The particle, on the other hand, should not be able to tell any difference. One important difference is that the probability distribution prefers some $x$ over others, that is, for $x \to Be^{-A}$, $P(x)$ grows.  

        In \citet{2014Flekkoy}, the process of random walking is described using the probability of going right denoted as $p$, and the probability of going left denoted $q$. Using the probaility distribution in the exercise where the probaility of going right is equal the probability of going left, following the understanding of the flat distribution, we can set $p = q = P(x)$. The mean displacement to the left is 
        \begin{equation}
            \langle R \rangle = Np = NP(x) = \frac{N}{2Ax}
            \label{eq:average_right}
        \end{equation}
\end{enumerate}
and the 

%%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%%%%%%%%
\bibliography{referanser}
\bibliographystyle{astroads}
%\bibliographystyle{astroads}
%\bibliographystyle{apj_hyperref}

\clearpage
\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Peebles equation}
\label{app:peebles}

%\lstinputlisting[language=c++]{../mainMonteCarloVMC1.cpp}

\end{document}

