\documentclass[a4paper,11pt]{article}
%\documentclass[preprint]{aa}

%\documentclass[preprint]{aastex}

%\documentclass[journal = ancham]{achemso}
%\setkeys{acs}{useutils = true}
%\usepackage{fullpage}
\usepackage{natbib,twoopt}
\pretolerance=2000
\tolerance=6000
\hbadness=6000
%\usepackage[landscape]{geometry}
%\usepackage{pxfonts}
%\usepackage{cmbright}
%\usepackage[varg]{txfonts}
%\usepackage{mathptmx}
%\usepackage{tgtermes}
\usepackage[utf8]{inputenc}
%\usepackage{fouriernc}
%\usepackage[adobe-utopia]{mathdesign}
\usepackage[T1]{fontenc}
%\usepackage[norsk]{babel}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage[version=3]{mhchem}
\usepackage{pstricks}
\usepackage[font=small,labelfont=bf,tableposition=below]{caption}
%\usepackage{subfig}
\usepackage{subcaption}
%\usepackage{varioref}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{sverb}
%\usepackage{microtype}
%\usepackage{enumerate}
\usepackage{enumitem}
%\usepackage{lineno}
%\usepackage{booktabs}
%\usepackage{changepage}
%\usepackage[flushleft]{threeparttable}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{mathtools}
%\usepackage{etoolbox}
%\usepackage{xstring}
\usepackage{aas_macros}

\floatstyle{plaintop}
\restylefloat{table}
%\floatsetup[table]{capposition=top}

\setcounter{secnumdepth}{3}

\newcommand{\tr}{\, \text{tr}\,}
\newcommand{\diff}{\ensuremath{\; \text{d}}}
\newcommand{\diffd}{\ensuremath{\text{d}}}
\newcommand{\sgn}{\ensuremath{\; \text{sgn}}}
\newcommand{\UA}{\ensuremath{_{\uparrow}}}
\newcommand{\RA}{\ensuremath{_{\rightarrow}}}
\newcommand{\QED}{\left\{ \hfill{\textbf{QED}} \right\}}

%% The below macros turn citations into ADS clickers in dvi, pdf, html output.
%% EDP Sciences improved them in December 2012 to work also with pdflatex.
\bibpunct{(}{)}{;}{a}{}{,}    %% natbib cite format used by A&A and ApJ
\makeatletter
 \newcommandtwoopt{\citeads}[3][][]{\href{http://adsabs.harvard.edu/abs/#3}%
   {\def\hyper@linkstart##1##2{}%
    \let\hyper@linkend\@empty\citealp[#1][#2]{#3}}}    %% Rutten, 2000
 \newcommandtwoopt{\citepads}[3][][]{\href{http://adsabs.harvard.edu/abs/#3}%
   {\def\hyper@linkstart##1##2{}%
    \let\hyper@linkend\@empty\citep[#1][#2]{#3}}}      %% (Rutten 2000)
 \newcommandtwoopt{\citetads}[3][][]{\href{http://adsabs.harvard.edu/abs/#3}%
   {\def\hyper@linkstart##1##2{}%
    \let\hyper@linkend\@empty\citet[#1][#2]{#3}}}      %% Rutten (2000)
 \newcommandtwoopt{\citeyearads}[3][][]%
   {\href{http://adsabs.harvard.edu/abs/#3}%
   {\def\hyper@linkstart##1##2{}%
    \let\hyper@linkend\@empty\citeyear[#1][#2]{#3}}}   %% 2000
\makeatother

%\newcommand{\diff}{%
%    \IfEqCase{frac{\diff}{%
%        {\ensuremath{frac{\text{d}} }}%
%        {\ensuremath{\; \text{d}} }% 
%    }[\PackageError{diff}{Problem with diff}{}]%
%}%


\date{\today}
\title{Compulsory assignment spring 2014\\ \small{Statistical mechanics -- FYS4130}}
\author{Marius Berge Eide \\ \texttt{m.b.eide@astro.uio.no}}


\begin{document}


\onecolumn
\maketitle{}


\section{Part 1}

\begin{enumerate}
    \item \textbf{Normalisation and variance}

        The distribution $P_0(x)$ is normalised;
        \begin{align*}
            \int_{-\infty}^{+\infty} P_0(x) \diff x &= \int_{-1}^{+1} \frac{1}{2} \diff x \\
            &= \frac{1}{2} \left[ 1 - \left( -1 \right) \right] = 1
        \end{align*}
        and the variance $\langle \Delta x^2 \rangle$ can be found as $\langle \Delta x^2 \rangle - \langle \Delta x \rangle^2$ with
        \begin{align*}
            \langle x \rangle &= \int_{-\infty}^{+\infty} x P_0(x) \diff x = \int_{-1}^{+1} x \frac{1}{2} \diff x \\
            &= \frac{1}{2} \left[ \frac{1}{2} \left( 1^2 - (-1)^2 \right) \right] = 0
        \end{align*}
        and
        \begin{align*}
            \langle x^2 \rangle &=  \int_{-\infty}^{+\infty} x^2 P_0(x) \diff x = \int_{-1}^{+1} x^2 \frac{1}{2} \diff x \\
            &= \frac{1}{2} \left[ \frac{1}{3} \left( 1^3 - (-1)^3 \right) \right] \\
            &= \frac{1}{6} 2 = \frac{1}{3}
        \end{align*}
        giving
        \begin{align}
            \langle \Delta x^2 \rangle &= \langle \Delta x^2 \rangle - \langle \Delta x \rangle^2 \notag \\
            &= \frac{1}{3} - 0 = \frac{1}{3}.
            \label{eq:variance1}
        \end{align}

    \item \textbf{Random sequence}
        A sequence of $N=2^{\left\{1,2,\dots,16  \right\}}$ $X$-values are produced\footnote{see the attached file \texttt{p1\_rng.py}}. The variance is plotted in fig.~(\ref{fig:12_variance}). 

        \begin{figure}[htb]
            \begin{center}
                \includegraphics[width=\columnwidth]{../12_variance.pdf}
            \end{center}
            \caption{Variance of random numbers $X$ produced by a random number generator in the range $[-1,1)$ following the flat distribution $P_0(x)$, plotted against number of random numbers $N$. Note how the variance stabilises with increasing $N$. }
            \label{fig:12_variance}
        \end{figure}

    \item \textbf{Histogram} A histogram  is plotted in fig.~(\ref{fig:13_histogram}). The central limit theorem states that, for high $k$, any distribution can be approximated by a normal distribution with variance $k \langle \Delta X^2 \rangle$ and mean $\langle X \rangle$.

        The shape of the histogram closely resembles that of a Gaussian distribution, and indicates that the statistic $\langle X \rangle$ is distributed normally, even though each statistic is made up from drawing $N$ random numbers from the flat distribution $P_0(x)$. 

        \begin{figure}[htb]
            \begin{center}
                \includegraphics[width=\columnwidth]{../13_histogram.pdf}
            \end{center}
            \caption{Histogram showing the distribution of the mean $\langle X \rangle$ from $k=200$ samples where each is made up of $N=2^{4}$ random numbers drawn from the flat distribution. Note how the histogram resembles a normal distribution.}
            \label{fig:13_histogram}
        \end{figure}
\end{enumerate}

\section{Part 2 -- The central limit theorem}
The first distribution to be examined is the power law approximation to the log-normal distribution;
\begin{equation}
    P(x) \propto \frac{1}{x}
    \label{eq:powerlawdist}
\end{equation}

\begin{enumerate}
    \item The distribution is not normalisable as
        \begin{equation}
            \int_{-\infty}^{+\infty} \frac{1}{x} \diff x = \infty
            \label{eq:diverging_dist}
        \end{equation}
        in other words, the integral diverges, and for that no normalisation constant exists. For $|x| \to 0$, $1/|x| \to \infty$, and for $|x| \to \infty$, $1/|x| \to 0$, so that cut off values must be introduced when implementing the distribution numerically (when is $|x|$ small enough to make $1/|x|$ approximately infinitely large, and inversely likewise for large $|x|$?).

    \item For a random variable $y$ distributed following the flat distribution $P_0(y)$ in between $-1$ and $+1$, a change of variable $y \to x$ must not change the probability over an infinitesimal small step;
        \begin{equation}
            P_0(y) \diffd y = P(x) \diffd x
            \label{eq:consv_prob}
        \end{equation}
        so that either distribution can be expressed in terms of the other,
        \begin{align}
            P(x) &= P_0(y) \frac{\diffd y}{\diffd x} \notag \\
            &= \frac{P_0(y)}{x'(y)}
            \label{eq:expdist_distilled}
        \end{align}
        where the prime ($'$) denotes derivative with respect to $y$, ie., $' \equiv \diffd / \diffd y$. 

    \item Using the change of variable $x = Be^{Ay}$ and the relation of eq.~(\ref{eq:expdist_distilled}), the $x$-dependent distribution can be written as
        \begin{align}
            P(x) &= \frac{P_0(y(x))}{x'(y)} = \frac{1}{2} \frac{1}{Ax},
            \label{eq:px}
        \end{align}
        where it was used that $x'(y) = \diffd (Be^{Ay})/\diffd y = ABe^{Ay} = Ax$. 

        The valid range for the distribution follows from the endpoints of the flat distribution. The minimum is given by $x(y=-1) = Be^{-A}$, and the maximum is given by $x(y=+1) = Be^{+A}$. For other $y$, the distribution is zero.

    \item The distribution of eq.~(\ref{eq:px}) is normalised,
        \begin{align*}
            \int_{-\infty}^{\infty} P(x) \diff x &= \int_{Be^{-A}}^{Be^{+A}} \frac{1}{2Ax} \diff x \notag \\
            &=  \frac{1}{2A}\int_{Be^{-A}}^{Be^{+A}} \frac{1}{x} \diff x = \frac{1}{2A} \left[ \ln \left( Be^{+A} \right) - \ln \left( Be^{-A} \right) \right] \notag \\
            &=  \frac{1}{2A} \left( A - (-A) \right) = 1
        \end{align*}
        and the variance can be found from $\langle x \rangle$, $\langle x^2 \rangle$;
        \begin{align}
            \langle x \rangle &= \int_{-\infty}^{\infty} x P(x) \diff x = \int_{Be^{-A}}^{Be^{+A}} x \frac{1}{2Ax} \diff x \notag \\
            &=\frac{1}{2A}\int_{Be^{-A}}^{Be^{+A}} \diff x = \frac{B}{2A} \left( e^{+A} - e^{-A}  \right) \notag \\
            &= \frac{B}{A} \sinh A
            \label{eq:mean_powerPX}
        \end{align}
        and
        \begin{align*}
            \langle x^2 \rangle &= \int_{-\infty}^{\infty} x^2 P(x) \diff x = \int_{Be^{-A}}^{Be^{+A}} x^2 \frac{1}{2Ax} \diff x \\
            &=\frac{1}{2A}\int_{Be^{-A}}^{Be^{+A}} x \diff x = \frac{1}{2A} \left[ \frac{1}{2} (Be^{+A})^2 - \frac{1}{2}(Be^{-A})^2  \right] \\
            &= \frac{B^2}{4A} \left[ e^{2A} - e^{-2A} \right] = \frac{B^2}{2A} \sinh \left( 2A \right) \\
            &= \frac{B^2}{A} \cosh A \sinh A
        \end{align*}
        giving the variance 
        \begin{align}
        \langle \Delta x^2 \rangle &= \langle x^2 \rangle - \langle x \rangle^2 \notag \\
        &= \frac{B^2}{A} \cosh A \sinh A - \frac{B^2}{A^2} \sinh^2 A \notag \\
        &= \frac{B^2}{A} \sinh A \left( \cosh A - \frac{1}{A} \sinh A \right)
            \label{eq:var_1}
        \end{align}

    \item The variance can be rewritten
        \begin{align}
            \langle \Delta x^2 \rangle &= \frac{B^2}{A^2} \sinh^2 A \left( A \frac{\cosh A}{\sinh A} -\frac{\sinh A}{\sinh A} \right) \notag \\
            &= \frac{B^2}{A^2} \sinh^2 A \left( A \coth A - 1 \right)
            \label{eq:var_2}
        \end{align}
        as $\tanh A = 1/\coth A$. 

    \item Calculating $k = 200$ sequences of $N$ random numbers distributed using the power law distribution of eq.~(\ref{eq:px}), with parameters $A= B= 1$, the mean (of the statistic mean) of all the sequences was found to be $\langle x \rangle = 0.5876$, and the mean of the variances was found to be $\langle \Delta x^2 \rangle = 0.1079$.

        The analytical mean, using eq.~(\ref{eq:mean_powerPX}) is $1.175$ and thus differs significantly from the computed mean.

        An attempt to plot the normal distribution,
        \begin{equation}
            P(x,\langle x \rangle, \langle \Delta x^2 \rangle) = \frac{1}{2 \pi N \langle \Delta x^2 \rangle} \exp \left[ - \frac{\left( x - \langle x \rangle \right)^2}{2 N \langle \Delta x^2 \rangle} \right]
            \label{eq:normal_dist}
        \end{equation}
        is shown in fig.~\ref{fig:26_powerdist}). Here, a problem is the scaling of the histogram, and that the mean of the two distributions are located at different locations. In order to be able to compare the two, the $N$-dependece of eq.~(\ref{eq:normal_dist}) is removed, and the histogram is scaled according to $k$, the number of sequences.

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\columnwidth]{../26_powerdist.pdf}
            \caption{Histogram showing the distribution of the statistic mean in $k=200$ samples, each with $N=1000$ elements drawn from the power-law distribution of eq.~(\ref{eq:px}), and a plot showing the normal distribution without $N$-dependence, centered around the analytical mean found in eq.~(\ref{eq:mean_powerPX}). There are obvious discrepancies. The mean of the two distributions are different, the variance of the computed distribution is much smaller than the analytical variance and the scaling of the histogram made it difficult to scale the normal distribution correctly. Here, the normal distribution is plotted without any $N$-dependence. }
            \label{fig:26_powerdist}
        \end{figure}

    \item Changing parameters to $A = 4$, $B = N = 100$, the results can be seen in fig.~(\ref{fig:27_powerdist})


        \begin{figure}[htb]
            \centering
            \includegraphics[width=\columnwidth]{../27_powerdist.pdf}
            \caption{Histogram showing the distribution of the means from $k= 200$ samplings of $N=100$ random numbers distributed according to eq.~(\ref{eq:px}) with parameters $A = 4$, and $B = 100$. No normal distribution is overplotted, as the analytical mean is $\langle X \rangle = 682.2$, and the mean of the computed statistic mean is $8.515 \times 10^{-2}$. The analytical variance is $N \langle \Delta X^2 \rangle = 1.398 \times 10^{8}$ and the computed variance is $2.165 \times 10^{-4}$.   }
            \label{fig:27_powerdist}
        \end{figure}<++>


    \item See fig.~(\ref{fig:28_hist}) for a histogram where the random numbers are distributed with parameters $A = 6$ and $B = N = 100$. See fig.~(\ref{fig:28_runningmean}) for a plot showing the running average of this histogram, where the averaging windows was $\Delta X = 10$.  

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\columnwidth]{../28_hist.pdf}
            \caption{Histogram with 100 bins of $k=200$ samples with parameters $A = 6$, $B = N = 100$. The high number of bins is required in order to make a running average over these values. See fig.~(\ref{fig:28_runningmean}). }
            \label{fig:28_hist}
        \end{figure}

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\columnwidth]{../28_runningmean.pdf}
            \caption{Running mean of computed histogram shown in fig.~(\ref{fig:28_hist}) with averaging window $\Delta X = 10$. The distribution appears to be slightly skewed, but does have the features of a Gaussian. The central limit theorem requires that the number of samples goes to infinity, that is $k \to \infty$, but in this case, $k=200$ and each sample consists of $N=100$ random numbers drawn from the power law distribution.    }
            \label{fig:28_runningmean}
        \end{figure}

    \item Considering a random walk where each step follows $P(x)$ from eq.~(\ref{eq:px}), with independent steps, the mean-sqaure displacement can be found by using known features of the flat distribution $P_0(y)$. In the interval $[-1,1]$, the probability of going left or right is independent of location (and thus previous steps) and given by $P_0(y) = 1/2$. Either you go left, or you go right.

        The change of variable allows us to retain the image of going left or right, but within a different interval and following a different distribution. The particle, on the other hand, should not be able to tell any difference. One important difference is that the probability distribution prefers some $x$ over others, that is, for $x \to Be^{-A}$, $P(x)$ grows.  

        In \citet{2014Flekkoy}, the process of random walking is described using the probability of going right denoted as $p$, and the probability of going left denoted $q$. Using the probaility distribution in the exercise where the probaility of going right is equal the probability of going left, following the understanding of the flat distribution, we can set $p = q = P(x)$. The mean displacement to the left is 
        \begin{equation}
            \langle R \rangle = Np = NP(x) = \frac{N}{2Ax}
            \label{eq:average_right}
        \end{equation}
and the average displacement $\langle S \rangle$ is
\begin{equation}
    \langle S \rangle = 4Npq = \frac{4N}{4A^2 x^2} = \frac{N}{A^2x^2}.
    \label{eq:avgdispl}
\end{equation}

However, the conceptual interpretation of the distribution $P(x)$ does not give a seemingly interpretable average displacement \textit{which has $x$-dependence}. In order to calculate the diffusion coefficient, I would need to know how the displacement relates to the distribution and the expressions derived earlier in this part.

\end{enumerate}

%%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%%%%%%%%
\bibliography{referanser}
\bibliographystyle{astroads}
%\bibliographystyle{astroads}
%\bibliographystyle{apj_hyperref}

%\clearpage
%\appendix
%\section{Appendix}
%\label{sec:appendix}
%
%\subsection{Peebles equation}
%\label{app:peebles}

%\lstinputlisting[language=c++]{../mainMonteCarloVMC1.cpp}

\end{document}
